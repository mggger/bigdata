{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建一个高可用的hdfs集群\n",
    "\n",
    "\n",
    "架构图 \n",
    "![architecture](1.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建步骤\n",
    "3台机器，和一个zookeeper集群(每台机器都启动了一个zookeeper服务).\n",
    "\n",
    "分别为master, node1, node2， 搭建想法是将master设为active namenode， node2 作为standby namenode, node1 为datanode\n",
    "\n",
    "***\n",
    "1. 下载[hadoop](http://apache.claz.org/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gz)\n",
    "2. 解压， 我解压的路径是/opt/bigdata/hadoop\n",
    "3. 申明JAVA_HOME环境变量\n",
    "4. 3台机器使用相同的配置, [hdfs-site.xml](hdfs-site.xml), [core-site.xml](core-site.xml)\n",
    "\n",
    "\n",
    "- 启动JournalNode服务. (每台机器都要执行)    \n",
    "    \n",
    "        hadoop-daemon.sh start journalnode\n",
    "        \n",
    "- namenode格式化   (master, node2执行即可)\n",
    "        \n",
    "        hdfs namenode -format\n",
    "- 启动namenode  （master)\n",
    "    \n",
    "        hadoop-daemon.sh start namenode\n",
    "        \n",
    "- 启动standby namenode (node2)\n",
    "\n",
    "        hdfs namenode -bootstrapStandby\n",
    "        hadoop-daemon.sh start namenode\n",
    "        \n",
    "- 在zookeeper中创建一个znode用来保存automatic failover的数据 (master, node2)\n",
    "    \n",
    "        hdfs zkfc –formatZK\n",
    "        \n",
    "- 启动DFSZKFailoverController服务\n",
    "        \n",
    "        hadoop-daemon.sh start zkfc\n",
    "        \n",
    "- 启动datanode (node1)\n",
    "        \n",
    "         hadoop-daemon.sh start datanode\n",
    "         \n",
    "- 使用cli查看namenode状态\n",
    "    \n",
    "        hdfs haadmin -getServiceState nn1\n",
    "        hdfs haadmin -getServiceState nn2\n",
    "         \n",
    "更详细的说明请参考\n",
    "\n",
    "[官方文档](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html)\n",
    "\n",
    "[How to Set Up Hadoop Cluster with HDFS High Availability](https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/)\n",
    "***\n",
    "\n",
    "添加新的节点步骤 \n",
    "- 复制所有的配置文件到新的节点\n",
    "- hadoop-daemon.sh start journalnode\n",
    "- hadoop-daemon.sh start datanode\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试\n",
    "\n",
    "异常测试:  杀死master的namenode, 观察变化\n",
    "\n",
    "1. 首先进入master的ui界面(http://master:50070)\n",
    "    ![master](master.png)\n",
    "    \n",
    "   node2的ui界面(http://node2:50070)\n",
    "   ![node2](node2.png)\n",
    "2. 在master上面输入jps 获取namenode 的进程号 并杀死, 重新启动master的namenode服务, 观察ui变化\n",
    "\n",
    "    ![master1](master-1.png)\n",
    "    \n",
    "    node2：\n",
    "    ![node21](node2-1.png)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yarn \n",
    "\n",
    "- 配置[yarn-site.xml](yarn-site.xml)\n",
    "- 修改资源节点 修改slaves文件\n",
    "- 启动sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他的发现\n",
    "\n",
    "**一个高可用的架构可以在不关闭服务情况下修改配置.**\n",
    "\n",
    "之前因为新增加节点遇到了\n",
    "\n",
    "```\n",
    "Datanode denied communication with namenode because hostname cannot be resolved \n",
    "```\n",
    "\n",
    "google后需要添加一条配置，先关闭nn1, 然后启动启动namenode服务, 再关闭nn2，启动namenode， 这样就在不关闭服务情况下修改配置."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
