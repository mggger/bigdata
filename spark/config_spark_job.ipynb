{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  spark on yarn 资源配置\n",
    "\n",
    "目前hadoop集群配置(1个master8核12g, 3个节点4核8g)\n",
    "\n",
    "资源调度算法.\n",
    "- 根据[经验](http://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/)每个executor占用的core在5或者5以下时候, hdfs吞吐量会拉满,所以每个executor的占用的核数应该在5以下 指定executor占用的核数数量为1\n",
    "- 每一个节点留一个核用于hadoop/yarn的后台服务. 每个节点可以利用的核数为4 - 1 = 3, 总共可以利用的核数为3 * 3 = 9\n",
    "- 总共可以有9 / 1 = 9 个executor\n",
    "- 留一个executor用于ApplicationManager, 剩下可以用的有9 - 1 = 8个\n",
    "- 每一个节点有的executor 8 / 3 = 2.6 个 (指定2个)\n",
    "- 每一个executor的可用的内存 8 / 2 = 4g\n",
    "- executor memory overhead 为可用内存的7%. 4g * 0.07 = 0.28g\n",
    "- 所以需要指定的executor-memory为 4 - 0.28 = 3.72g (3.5g)\n",
    "\n",
    "所以推荐的配置为\n",
    "2个executor， 每个executor 3.5g， 每个executor一个核\n",
    "\n",
    "这种配置方法，能够保证一个spark任务在一台机器上面运行， 不会跨机器\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
